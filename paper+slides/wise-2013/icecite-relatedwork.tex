\section{Related Work}\label{sec:related_work}
We split the related work section mainly in three parts. In the first part, we introduce existing \textit{desktop-based} and \textit{web-based applications} similar to Icecite. In the second part, we focus on existing \textit{extraction techniques} to retrieve informations about the metadata and/or the bibliographic references from research papers. 
Because we use a metadata knowledge base in background and match an extract to the referred record in the knowledge base to get full and correct metadata, we finally focus in the third part on \textit{record matching techniques}.

\subsection{Related Applications}
A lot of applications were already developed, aimed to support researchers during a literature research process. In this section, we introduce some of them and compare their feature sets. Table \ref{tab:featureoverview} summarizes the individual feature sets in an overview.  

\subsubsection{Desktop-based Applications}
The upper part of Table \ref{tab:featureoverview} breaks down the provided features of \textit{desktop-based} applications. All of them can be seen as \textit{research managers} and are quite similar regarding to their basic structure: research paper are manageable in individual libraries, where they are listed with their (usually automatically identified) metadata. However further features differ from application to application. Because we can't describe all applications in detail here, we pick a single representative one, namely \textit{Mendeley} \cite{www:mendeley}. We have chosen Mendeley, because it considers itself to be "the world's largest social reference management system" \cite{DBLP:conf/www/KrakerKJG12}. Moreover we believe that its feature set comes closest to the features of a state-of-the-art research manager.
In the following, we will examine Mendeley's feature set in detail and compare it with those of the other applications.  

%\begin{table*}[!ht]
%\centering
%\renewcommand{\arraystretch}{1.3}
%\setlength{\tabcolsep}{1pt}
%\begin{tabular}{l|cccccccccccccccc}
%\textit{Name of application }	& \thead{(WEB)} & \thead{(EX-M)} & \thead{(EX-R)} & \thead{(PDF)} & \thead{(ANN)} & \thead{(TAG)} & \thead{(S-PL)} & \thead{(S-FT)} & \thead{(S-DL)} & \thead{(IMP-DL)} & \thead{(IMP-R)} & \thead{(RES-PDF)} & \thead{(SY-M)} & \thead{(SY-P)} & \thead{(SY-A)} & \thead{(SH)}\\ \hline
%Citavi \cite{www:citavi} 			& \no & \cond & \no & \yes & \no & \yes & \yes & \no & \yes & \yes & \no & \cond & \no & \no & \no & \cond   \\
%EndNote \cite{www:endnote} 		& \no & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? \\
%EverNote \cite{www:evernote}	& \cond & \no & \no & \yes & \no & \yes & \yes & \yes & \no & \no & \no & \no & \yes & \yes & \no & \yes \\
%Mendeley \cite{www:mendeley}	& \cond  & \yes & \no & \yes & \yes & \yes & \yes & \yes & \no & \cond  & \no & \no & \yes & \yes & \yes & \yes  \\
%Papers \cite{www:papers}			& \no & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? & ? \\
%Qiqqa \cite{www:qiqqa}				&	\cond & \yes & \no & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \no & \no & \yes & \yes & ? & \yes \\
%ReadCube \cite{www:readcube} 	& \no & \yes & \cond & \yes & \yes & \no & \yes & \yes & \yes & \yes & \cond & \yes & \no & \no & \no & \no  \\
%Zotero \cite{www:zotero}			& \cond & \yes & \no & \yes & \no & \yes & \yes & \no & \no & \yes & \no & \no & \yes & \yes & \no & \yes \\ \hline
%BibSonomy \cite{www:bibsonomy}& \yes & \no & \no & \yes & \no & \no & \yes & \no & \yes & \yes & \no & \no & \no & \no & \no & \yes \\
%CiteULike \cite{www:citeulike}& \yes & \no & \no & \yes & \cond & \yes & \yes & \yes & \yes & \yes & \no & \no & \no & \no & \no & \yes\\
%EndNote Web \cite{www:endnoteweb} & \yes & \no & \no & \no & \no & \yes & \yes & \no & \yes & \yes & \no & \no & \no & \no & \no & \yes \\
%RefWorks \cite{www:refworks} 	& \yes & \no & \no & \no & \no & \no & \yes & \no & \yes & \yes & \no & \no & \no & \no & \no & \yes \\
%A.nnotate \cite{www:annotate}	& \yes & \no & \no & \yes & \yes & \yes & \yes & \no & \no & \no & \no & \no & \no & \no & \no & \yes \\
%Crocodoc Personal \cite{www:crocodoc}	& \yes & \no & \no & \yes & \yes & \no & \no & \no & \no & \no & \no & \no & \no & \no & \no & \yes \\
%WebNotes \cite{www:webnotes}	& \yes & \no & \no & \yes & \yes & \yes & \yes & \yes & \no & \no & \no & \no & \no & \no & \no & \yes \\
%\end{tabular}
%\caption{Overview of the provided features by applications related to Icecite. If a feature is fully provided by an application, it's labeled with an checkmark. If a feature is only partially provided, it's labeled with an circle. The upper part of the table denotes desktop-based, the lower part web-badsed applications. Each acronym in the header of the table denotes an specific feature. The meanings of them are as follows: \textit{(WEB)}: application is web-based; \textit{(EX-M)}: extraction of metadata from PDF files; \textit{(EX-R)}: extraction of references from PDF file; \textit{(PDF)}: management of PDF files; \textit{(ANN)}: annotating PDF files; \textit{(TAG)}: tagging documents in the personal library; \textit{(S-PL)}: searching the personal library; \textit{(S-FT)}: searching the fulltexts; \textit{(S-ES)}: searching in external sources; \textit{(IMP-ES)}: Importing documents from external sources into personal library; \textit{(IMP-R)}: Importing referenced documents into the personal library; \textit{(RES-PDF)}: resolving PDF files for external documents; \textit{(SY-M)}: Synchronization of metadata; \textit{(SY-P)}: Synchronization of PDF files; \textit{(SY-A)}: Synchronization of annotations; \textit{(SH)}: Sharing documents of personal library with other users.}
%\label{tab:featureoverview}
%\end{table*}

\begin{table*}[!ht]
\centering
\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{1pt}
\begin{tabular}{l|cccccccc}
\textit{Name of application }	& \thead{(CLOUD)} & \thead{(OFFLINE)} & \thead{(SHARED)} & \thead{(EX-M)} & \thead{(EX-R)} & \thead{(AUTO-DL)} & \thead{(ANNOT)} & \thead{(SEARCH)} \\ \hline
Citavi \cite{www:citavi} 			& \no & \yes & \cond & \cond & \no & \cond & \no & \cond \\
EndNote \cite{www:endnote} 		& ? & ? & ? & ? & ? & ? & ? & ? \\
EverNote \cite{www:evernote}	& \yes & \yes & \yes & \no & \no & \no & \no & \cond \\
Mendeley \cite{www:mendeley}	& \yes & \yes & \yes & \yes & \no & \no & \yes & \cond \\
Papers \cite{www:papers}			& ? & ? & ? & ? & ? & ? & ? & ? \\
Qiqqa \cite{www:qiqqa}				&	\yes & \yes & \yes & \yes & \no & \no & \yes & \yes  \\
ReadCube \cite{www:readcube} 	& \no & \yes & \no & \yes & \cond & \yes & \yes & \yes  \\
Zotero \cite{www:zotero}			& \yes & \yes & \yes & \yes & \no & \no & \no & \cond \\ \hline
BibSonomy \cite{www:bibsonomy}& \yes & \no & \yes & \no & \no & \no & \no & \no \\
CiteULike \cite{www:citeulike}& \yes & \no & \yes & \no & \no & \no & \cond & \yes \\
EndNote Web \cite{www:endnoteweb} & \yes & \no & \yes & \no & \no & \no & \no & \cond \\
RefWorks \cite{www:refworks} 	& \yes & \no & \yes & \no & \no & \no & \no & \cond \\
A.nnotate \cite{www:annotate}	& \yes & \no & \yes & \no & \no & \no & \yes & \cond \\
Crocodoc Personal \cite{www:crocodoc}	& \yes & \no & \yes & \no & \no & \no & \yes & \no \\
WebNotes \cite{www:webnotes}	& \yes & \no & \yes & \no & \no & \no & \yes & \cond \\
\textbf{Icecite} 							& \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
\end{tabular}
\caption{Overview of the provided features by applications related to Icecite. If a feature is fully provided by an application, it's labeled with an checkmark. If a feature is only partially provided, it's labeled with an circle. The upper part of the table denotes desktop-based, the lower part web-badsed applications. Each acronym in the header of the table denotes an specific feature. The meanings of them are as follows: \textit{(WEB)}: application is web-based; \textit{(EX-M)}: extraction of metadata from PDF files; \textit{(EX-R)}: extraction of references from PDF file; \textit{(PDF)}: management of PDF files; \textit{(ANN)}: annotating PDF files; \textit{(TAG)}: tagging documents in the personal library; \textit{(S-PL)}: searching the personal library; \textit{(S-FT)}: searching the fulltexts; \textit{(S-ES)}: searching in external sources; \textit{(IMP-ES)}: Importing documents from external sources into personal library; \textit{(IMP-R)}: Importing referenced documents into the personal library; \textit{(RES-PDF)}: resolving PDF files for external documents; \textit{(SY-M)}: Synchronization of metadata; \textit{(SY-P)}: Synchronization of PDF files; \textit{(SY-A)}: Synchronization of annotations; \textit{(SH)}: Sharing documents of personal library with other users.}
\label{tab:featureoverview}
\end{table*}


Mendeley is basically a desktop-based application. Though Mendeley comes with a web-interface to browse and manage the library data in an ordinary web browser, which were synchronized with Mendeley's server. However the number of features of the web interface is restricted and that's why there is a '\cond' in first column of Table \ref{tab:featureoverview} for Mendeley (a similar argument is valid for EverNote and Qiqqa). Further, Zotero comes with an extension for the Firefox browser.
 
In Mendeley, the metadata of research papers are extracted automatically from the PDF file. In contrast, the extraction of bibliographic references from PDF files isn't provided, neither in Mendeley\footnote{Consider, that the extraction of references was supported in a previous version of Mendeley, but was removed again, "because it was consuming a fair amount of resources (on client and server side) without providing enough value" \TODO{How to cite the referred blog post?}} nor in other research managers. Consider, that ReadCube provides a feature called \textit{enhancing PDF files}. For example, this feature enhance particular PDF files with prepared informations about bibliographic references. However, ReadCube doesn't extract these data directly from PDF files, but tries to resolve the DOI (digital object identifier) of a document and gathers the data from any web pages afterwards. Hence prepared references are only available, when the document contains such a DOI and when the data are retrievable from the web.

Further features of Mendeley are the built-in PDF viewer to read and to annotate PDF files within the application and the chance to tag documents with specific keywords. All data including the fulltexts of documents can be searched to find particular entries in the library. However external sources (like digital libraries) aren't browseable within Mendeley, as it is possible within Citavi, Qiqqa and ReadCube.  

Furthermore, a bookmarklet (the so called \textit{Web importer}) can be installed to the ordinary web browser to import the metadata of research papers from several external sources into the personal library, along with the corresponding PDF file, if it's provided by the chosen source. But the weaknesses of this feature are (1) the need to switch between Mendeley's interface and the browser and (2) the lack of an automatic search for PDF files in case that the external source doesn't provide PDF files. In contrast, ReadCube searches for PDF files on browsing the external sources to import them into the library with a single click. If a PDF could be \textit{enhanced} (see above), this feature is also used to import a reference along with its PDF file into the library easily. However the approach to import documents by references is very simplistic, because the retrieved reference string can be just sent to an external source as query, in the hope that the correct record will be found in the external source.

As mentioned above, Mendeley can synchronize the library data with its server to access them via the provided web interface or from several local installations of Mendeley on various devices. Moreover, the data synchronization offers the chance to share them with other members of individual groups to collaborate.

\TODO{Explain some special features? For example the feature of ReadCube: list all citing papers} 

\subsubsection{Web-based Applications}
The lower part of Table \ref{tab:featureoverview} breaks down the provided features of \textit{web-based} applications. Compared to desktop-based applications, their amount is usually much smaller. Most of the web-based applications focus only on specific subareas of literature research, like \textit{pure metadata managing} or \textit{PDF file annotating}. 

Pure metadata managing is for example supported by \textit{BibSonomy}, \textit{CiteULike}, \textit{EndNote Web} and\textit{RefWorks} \cite{www:refworks}. Basically, each application allows to collect bibliographic metadata systematically in a personal library and to generate bibliographies for own publications from them. A record can be either created manually or imported from external sources. All metadata records are generally fully searchable. Furthermore, libraries are typically shareable with other users to get a combined set of metadata. Two of the listed applications (namely BibSonomy and CiteULike) allow to attach PDF files to each record, whereas the automatic extraction of metadata and references is generally not supported. Further, annotating PDF files is possible seldomly because only CiteULike provides this feature in a premium version for paying users. Though, annotating PDF files in the browser is the aim of the following type of web-based applications.
 
Annotating PDF file in the browser is for example supported by \textit{A.nnotate}, \textit{Crocodoc} or \textit{WebNotes}. All applications allow to read, annotate and comment PDF files collaboratively. The features are usable via an ordinary web browser due to techniques of HTML5 or Flash. Basically, the scopes of supported files are not limited to PDF files, but also include arbitrary text files, images and even websites. Hence, such applications are not primarily intended to support a literature research process. That's why features like \textit{automatic metadata extraction from PDF files} or \textit{automatic search for fulltexts} are usually missing in annotation applications \\

\subsubsection{Comparison of web-based and desktop-based applications}
\TODO{Wrote something about our aim, e.g. providing same scope of features, that are currently provided by desktop-based applications}.
%We have seen that a lot of features were already implemented, aimed to automatize parts of the literature research process. However, we think that there is still room for more supporting features. For example, there is no application, that provides a feature to identify the metadata of the references of research papers, although they offer a great potential to find related papers. 

%\TODO{More missing features}.
%\TODO{Explain the weakness of approaches of existing applications}. 

\subsection{Metadata Extraction Techniques}
In this section, we focus on related techniques to extract the metadata fields (title, author(s), affiliation(s), journal, etc.) of research papers and their bibliographic references. At first we will introduce some machine learning techniques, namely \textit{Hidden Markov Models}, \textit{Support Vector Machines} and \textit{Conditional Random Fields} and analyze their extraction accuracy. Afterwards, we will do the same with rule-based techniques 
% and examine, which techniques are used by the applications introduced above.

\subsubsection{Hidden Markov Models (HMMs)}
Hidden Markov Models have been applied to identify the metadata fields of both, research papers and their references. Seymore et al   \cite{Seymore99LearningHMM} describe an approach, where a manually-constructed model are used to identify the metadata fields in the headers of research papers. The constructed model contains multiple states per field. They achieve an extraction accuracy of 92.9\% over all metadata fields. 

Borkar et al \cite{DBLP:conf/sigmod/BorkarDS01} use a nested HMM to identify the metadata fields of references. The constructed HMM contains one state per metadata field. Each state itself consists of another HMM, representing the internal structure of the field. With this method, Borkar et al achieve an average precision and recall ratio of 0.87 over all fields.

\subsubsection{Support Vector Machines (SVMs)}
Support Vector Machines are used by Han et al \cite{DBLP:conf/jcdl/HanGMZZF03} to identify the metadata fields in the headers of research papers. Their experiments based on 500 training headers and 435 test headers resulted in an overall accuracy of 92.9\%.

The metadata extraction of Mendeley relies on a two-stage SVM and is aimed to extract the title and the authors from research papers (Granitzer et al, \cite{DBLP:conf/sac/GranitzerHJK12}). In a first step, it classifies each line of the header text into title, author and other classes using text and formating features. In a second step, the classification is 
improved by using contextual information such as the predicted class labels of the neighboring lines. The experiments of Granitzer et al. based on a training set of 1000 research papers. They resulted in an average precision rate of 0.81 and an average recall rate of 0.62 for the author extraction and in an average precision rate of 0.94 and an average recall rate of 0.91 for the title extraction.

\subsubsection{Conditional Random Fields (CRFs)} 
Peng et al. \cite{DBLP:conf/naacl/PengM04} utilizes Conditional Random Fields to extract the metadata from both, the headers and the references of research papers. Experiments to test the field extraction from headers were performed on 500 training headers and 435 test headers and resulted in an overall accuracy of 73.3\%. Experiments to test the field extraction from references were performed on the \textit{Cora} dataset, containing 500 references. They resulted in an overall extraction accuracy of 77.3\%. \TODO{The claimed numbers are instance accuracies: the percentage of instances in which every word is correctly labeled. Word accuracies is quite higher} \\

Machine learning techniques are generally considered to be robust and adaptable \cite{DBLP:conf/jcdl/HanGMZZF03}. Moreover, the above announced extraction accuracies show, that utilizing machine learning techniques is a promising way to extract metadata from research papers and their references.

However as claimed by Guo and Jin \cite{DBLP:conf/pdcat/GuoJ11}, generating accurate labeled datasets needed to train the models is time-consuming and costly. Alternatives to machine-learning approaches are rule-based approaches, which are usually faster, but less accurate.

\subsubsection{Rule-based approaches}
Rule-based approaches base on a set of rules, describing how to identify the metadata to extract in research papers. The rules are deduced from human observations regarding the basic structures of research papers.

Jöran Beel et al. \cite{DBLP:conf/ercimdl/BeelGSF10} have identified the titles of 693 research papers by using a rule-based approach, i.e. by utilizing their layout informations, i.e. the font sizes and their positions within the documents. Their experiments yield to 77,9\% correctly identified titles.

Cortez et al \cite{DBLP:conf/jcdl/VilarinhoSGMM07} propose a unsupervised knowledge-based approach to recognize the components of reference strings given in any format. The authors split each reference string into blocks and label each block with the name of a metadata field. Therefore a knowledge is used, that was constructed automatically from an existing set of sample metadata records. Experiments were performed on 300 reference strings and have produced an overall extraction accuracy of 82\%. \TODO{Verify this accuracy}.

All approaches seen so far try to extract any metadata fields of research papers and/or their references without utilizing any metadata knowledge bases to guide the extraction process. Guo and Jin introduce this approach in \cite{DBLP:conf/pdcat/GuoJ11}. They extract metadata using a rule-based approach and check if there is any matched document in a knowledge base, which is mainly developed from DBLP. If matched, they use the metadata stored in the knowledge base to make sure the extracted reference metadata are correct. The experiments on 97 research papers and 2157 references to be extracted resulted in an average extraction accuracy of 89,1\% over all metadata fields.
 
Basically, our approach follows up the approach of Guo and Jin. In a first step, the title and each reference-string is extracted from a research paper, profiting from the speed advantage of a simple rule-based approach compared to a machine learning approach. In a second step, each extract is matched to a record of DBLP or Medline, to get full and correct metadata. That's why we inspect existing record matching techniques in the following.   

\subsection{Record Matching Techniques} \label{sec:relatedwork:recordmatching}
As described above, we match the extracted data to a record of a metadata knowledge base to get full and correct metadata. In this section, we inspect the most common record matching techniques.
   
For our purposes, record matching can be defined in the following way: Given an extract (i.e. the title or a reference of a research paper), that refer to a record in the metadata knowledge base. Identify this record and associate it with the given extract. The task may be affected by noisy factors (like extraction failures, different spellings, abbreviations, etc.), whereby the typographic design of a metadata field may differ from the specifications in the referred record. 

Most of the existing record matching techniques base on string comparison approaches, which generally determines the similarity of two strings on the base of character-based, token-based or phonetic-based similarity measures. We only focus on character-based measures here. For a detailed discussion of all the measures, see the surveys in \cite{DBLP:journals/tkde/ElmagarmidIV07} and \cite{DBLP:journals/cacm/KanT08}.

Character-based similarity measures analyzes the similarity of two given strings $s$ and $t$ by comparing the strings character by character. For example, the \textit{Levenshtein distance} \cite{levenshtein} between $s$ and $t$ is defined as the minimal number of needed edit operations (\textit{add}/\textit{replace}/\textit{delete} a character) to transform $s$ into $t$. The Levenshtein distance is quite reasonable to match an extracted title to its referred record, because the expected similarity between the extracted title and the record's title is quite high. Apart from that, the Levenshtein distance isn't reasonable to match an extracted reference to its referred entity, because of the multiple metadata fields and their unknown order in the reference string. Hence, the expected similarity between an arbitrary combination of the metadata fields of an record and the extracted reference is usually low and thus mostly insignificant. 

Another example of a character-based measure is the \textit{Smith-Waterman distance} \cite{smithwaterman}, an extension of the Levenshtein distance, which was introduced to find the best matching substrings between $s$ and $t$.
For example, the Smith-Waterman distance is quite reasonable to decide, if specific metadata fields (e.g. the title) of an record is included in a given reference string. So, the referred record may be identified by the number of the record's fields included in the given reference string.

Because the complexity of both algorithm is $O(|s|\cdot|t|)$, brute-force pairwise comparisons over all records to identify the referred record by a given extract are usually not feasible. Instead it's quite common to segment the set of records into \textit{blocks} in a pre-processing step \cite{DBLP:journals/cacm/KanT08}. This segmentation usually relies on some weak and non-costly criteria. The costly string similarity measures are then only executed on single blocks of entity candidates.

However choosing a reasonable blocking strategy, i.e. selecting attributes for blocking is a challenging task. Michelson and Knoblock suggest a machine learning approach, that learns blocking schemes automatically, see 
\cite{DBLP:conf/aaai/MichelsonK06}.